{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6adf297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('df_final.csv')\n",
    "\n",
    "files  = df['img'].values\n",
    "labels = df['has_cancer'].values.astype(bool)\n",
    "\n",
    "train_files, test_files, y_train, y_test = train_test_split(files, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "class CancerDataset(Dataset):\n",
    "    def __init__(self, file_paths, targets, transform=None):\n",
    "        self.file_paths = [f\"data_raw/{file_path}\" for file_path in file_paths]\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "    \n",
    "    # necesitamos saber la longitud para luego saber cuantos batches vamos a necesitar\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.file_paths[index]).convert('L')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        targets = self.targets[index]\n",
    "\n",
    "        return image, targets\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # reducimos tamaño\n",
    "    transforms.ToTensor(),          # convierte a tensor y normaliza a [0,1]\n",
    "])\n",
    "    # transforms.ToTensor() ya hace esto\n",
    "# Con unsqueeze le añado una dimensión en la posicion 0 con un 1, que es la dimensión del color donde le decimos al modelo que solo hay uno, es decir gris\n",
    "# dividimos los pixeles entre 255 para tener valores noramlizados entre 0 y 1\n",
    "train_dataset = CancerDataset(train_files, y_train, transform=transform)\n",
    "test_dataset = CancerDataset(test_files, y_test, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175b946d",
   "metadata": {},
   "source": [
    "Hay que pasar todo a tensores para que el modelo pueda ser entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da9f01a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62a79463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — Loss: 0.3140 — Val Acc: 93.31%\n",
      "Epoch  2 — Loss: 0.2022 — Val Acc: 95.57%\n",
      "Epoch  3 — Loss: 0.1480 — Val Acc: 96.53%\n",
      "Epoch  4 — Loss: 0.1328 — Val Acc: 96.61%\n",
      "Epoch  5 — Loss: 0.1151 — Val Acc: 96.78%\n",
      "Epoch  6 — Loss: 0.1089 — Val Acc: 96.38%\n",
      "Epoch  7 — Loss: 0.0964 — Val Acc: 96.06%\n",
      "Epoch  8 — Loss: 0.0937 — Val Acc: 97.44%\n",
      "Epoch  9 — Loss: 0.0893 — Val Acc: 97.29%\n",
      "Epoch 10 — Loss: 0.0858 — Val Acc: 97.46%\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1    = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.bn1      = nn.BatchNorm2d(32)\n",
    "        self.conv2    = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2      = nn.BatchNorm2d(64)\n",
    "        self.pool     = nn.MaxPool2d(2, 2)\n",
    "        self.adapt    = nn.AdaptiveAvgPool2d((8, 8))\n",
    "        self.fc1      = nn.Linear(64 * 8 * 8, 256)\n",
    "        self.fc2      = nn.Linear(256, 128)\n",
    "        self.fc3      = nn.Linear(128, 1)\n",
    "        self.dropout  = nn.Dropout(0.5)\n",
    "        self.relu     = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.adapt(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x.squeeze(1)\n",
    "\n",
    "model = CNN().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs  = 10\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # — Training —\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)                     # [B,1,256,256]\n",
    "        labels = labels.float().to(device)             # BCEWithLogits expects float\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images).view(-1)                # [B]\n",
    "        loss   = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # — Validation —\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(images).view(-1)\n",
    "            preds  = (torch.sigmoid(logits) > 0.5).long()\n",
    "            correct += (preds == labels.long()).sum().item()\n",
    "            total   += labels.size(0)\n",
    "    accuracy = correct / total * 100\n",
    "    \n",
    "    print(f\"Epoch {epoch:2d} — Loss: {epoch_loss:.4f} — Val Acc: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7e3e8a",
   "metadata": {},
   "source": [
    "Guardo los pesos, luego para llamarlo desde fuera tengo que nombrar al modelo \n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "state_dict = torch.load(\"reconocimientoCancer.pth\", map_location=device)\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba4671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./streamlit/reconocimientoCancer.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
